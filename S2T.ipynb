{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6946f604-a366-47e1-a745-02b4628e9f2e",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install jiwer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974c5a0a-eafd-45a7-819d-3c0077ec54f9",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4eba9f-d52b-467c-9030-414aa4cebf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "##importing the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "!pip install tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f3c190-05d0-4104-85a2-143b0d27861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\"\n",
    "## extracting the data of LJspeech dataset using the keras in tensorflow\n",
    "## untar = True is used to extract the file if it is archived\n",
    "data_path = keras.utils.get_file(\"LJSpeech-1.1\", data_url, untar =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5784a7c-c9b5-4abe-925f-afd1498fe352",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c774e2d-cda1-4426-92cc-4e34e4572393",
   "metadata": {},
   "outputs": [],
   "source": [
    "wavs_path = data_path + \"/LJSpeech-1.1/wavs/\"\n",
    "metadata_path = data_path + \"/LJSpeech-1.1/metadata.csv\"\n",
    "##converting the csv file to a dataframe using pandas\n",
    "metadata_df = pd.read_csv(metadata_path, sep = \"|\", header = None, quoting = 3)\n",
    "metadata_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903dd194-2f3a-4e72-823c-6b8615325f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df.columns = [\"file name\", \"transcription\", \"normalized transcription\"]\n",
    "##reshuffling the rows of the metadata df in a random order and not not\n",
    "\n",
    "metadata_df = metadata_df.sample(frac = 1).reset_index (drop = True)\n",
    "metadata_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f23c28-93b5-4f29-86ae-6d25d82a0aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## splitting the dataframe into two parts : training (90%), test (10%); using int to get an index at which to split\n",
    "split = int(len(metadata_df) *0.0010)\n",
    "df_train = metadata_df [:split]\n",
    "df_test = metadata_df[split:]\n",
    "print (\"size of the training dataframe : \",{len(df_train)})\n",
    "print (\"size of the test dataframe : \",{len(df_test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89325021-693b-4b29-86a4-8f624b003fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a list of allowed vocabulary\n",
    "characters = [x for x in \"abcdefghijklmnopqrstuvwxyz'?!\"]\n",
    "#converting the charactrs to integer values using keras; any character not in the list is given an empty string\n",
    "char_to_num = keras.layers.StringLookup(vocabulary = characters, oov_token = \"\")\n",
    "#converting the integer back to the character using keras, specifying it using invert\n",
    "num_to_char = keras.layers.StringLookup(vocabulary = char_to_num.get_vocabulary(), oov_token = \"\", invert = True)\n",
    "print (char_to_num.get_vocabulary())\n",
    "print(char_to_num.vocabulary_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b870853-ffda-437e-97c2-2320166afb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the frame length, frame step and forward fourier transform rate\n",
    "frame_length = 256\n",
    "frame_step = 160\n",
    "fft_length = 384\n",
    "#defining function for getting a spectrogram of the audio files and its label\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def encode_single_sample (wav_file, label):\n",
    "  #reading the audio file\n",
    "  file = tf.io.read_file (wavs_path + wav_file + \".wav\")\n",
    "  #decoding the audio file using audio.decode_wav from the TensorFlow package to a float tensor\n",
    "  audio,_ = tf.audio.decode_wav(file)\n",
    "  #removing dimensions of size 1 from the tensor\n",
    "  audio = tf.squeeze(audio, axis = -1)\n",
    "  #changing the data type\n",
    "  audio = tf.cast(audio, tf.float32)\n",
    "  #converting the audio signal into a time frequency representation using Short Time Fourier Transfrom\n",
    "  spectrogram = tf.signal.stft (audio, frame_length = frame_length, frame_step = frame_step, fft_length = fft_length)\n",
    "  #calculating the magnitude of the spectrogram\n",
    "  spectrogram = tf.abs(spectrogram)\n",
    "  #normalizing the power\n",
    "  spectrogram = tf.math.pow(spectrogram, 0.5)\n",
    "  #performing the standardization of the spectrogram\n",
    "  means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n",
    "  stddev = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n",
    "  spectrogram = (spectrogram - means)/(stddev - 1e-10)\n",
    "  #splitting the label character and converting it to a numerical representation\n",
    "  label = tf.strings.lower(label)\n",
    "  label = tf.strings.unicode_split(label, input_encoding = \"UTF-8\")\n",
    "  label = char_to_num(label)\n",
    "  return spectrogram, label\n",
    "batch_size = 32\n",
    "file_names = np.array(df_train[\"file name\"])\n",
    "transcriptions = np.array(df_train[\"normalized transcription\"])\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(( file_names,transcriptions))\n",
    "train_dataset = (train_dataset.map(encode_single_sample, num_parallel_calls = tf.data.AUTOTUNE).padded_batch(batch_size).prefetch(buffer_size = tf.data.AUTOTUNE))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((file_names, transcriptions))\n",
    "test_dataset = (test_dataset.map(encode_single_sample, num_parallel_calls = tf.data.AUTOTUNE).padded_batch(batch_size).prefetch(buffer_size = tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c51f984-9444-4725-99a4-e9d60813a168",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining function for CTCloss; y_true--> target , y_pred--> predicted\n",
    "def CTCloss (y_true, y_pred):\n",
    "  #getting the shape of the tensor and extracting the first dimension\n",
    "  batch_len = tf.cast(tf.shape(y_true)[0], dtype = \"int64\")\n",
    "  #calculating the second dimension\n",
    "  input_length = tf.cast(tf.shape(y_pred)[1], dtype = \"int64\")\n",
    "  #calculating the length of the label sequence\n",
    "  label_length = tf.cast(tf.shape(y_true)[1], dtype = \"int64\")\n",
    "  #giving it shape (batch_len, 1), creating a 2D tensor with ones\n",
    "  input_length = input_length * tf.ones(shape = (batch_len, 1), dtype = \"int64\")\n",
    "  label_length = label_length * tf.ones(shape = (batch_len, 1), dtype = \"int64\")\n",
    "  #calculating the CTC loss\n",
    "  #keras is deeplearning framework(API) in the TensorFlow package\n",
    "  #the backend function does the mathematical operations on the tensors\n",
    "  loss = keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1057bc87-9dfb-4ce0-8230-627a4e4fa2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##defining a neural network using keras\n",
    "##inputdim = dimension of the input\n",
    "##outputdim = dimension of the input\n",
    "##rnn layers = number of recurrent layers in the network\n",
    "##rnn units = number of neurons\n",
    "def draft_model (inputdim, outputdim, rnn_layers = 5, rnn_units = 128):\n",
    "    #allowing the input to have variable length \n",
    "  input_spectrogram = layers.Input ((None, inputdim), name = \"input\")\n",
    "#reshaping \n",
    "  x = layers.Reshape((-1, inputdim, 1), name = \"expanddim\")(input_spectrogram)\n",
    "    #adding a convolutional layer\n",
    "  x = layers.Conv2D( filters = 32, kernel_size = [11,41], strides = [2,2], padding = \"same\", use_bias = False, name = \"conv_1\",)(x)\n",
    "#normalizing the layers \n",
    "  x = layers.BatchNormalization(name = \"conv_1_bn\")(x)\n",
    "    #adding non-linearity using ReLU (Rectified Linear Unit)\n",
    "  x = layers.ReLU(name = \"conv_1_relu\")(x)\n",
    "#adding convolutional layer\n",
    "  x = layers.Conv2D(filters = 32, kernel_size =[11,21], strides = [1,2], padding = \"same\", use_bias = False, name = \"conv_2\")(x)\n",
    "  x = layers.BatchNormalization(name =\"conv_2_bn\")(x)\n",
    "  x = layers.ReLU(name = \"conv_2_relu\")(x)\n",
    "    #falttening the tensor into a 2D tensor\n",
    "  x = layers.Reshape((-1, x.shape[-2]*x.shape[-1]))(x)\n",
    "#Building a stack of birectional GRU (gated recurrent unit) layers  \n",
    "  for i in range (1, rnn_layers + 1):\n",
    "    recurrent = layers.GRU(units= rnn_units, activation = \"tanh\", recurrent_activation = \"sigmoid\", use_bias = True, return_sequences = True,\n",
    "                           reset_after = True, name = f\"gru_{i}\",)\n",
    "    x = layers.Bidirectional(recurrent, name = f\"bidirectional_{i}\", merge_mode=\"concat\")(x)\n",
    "    if i < rnn_layers:\n",
    "      x = layers.Dropout(rate= 0.5)(x)\n",
    "    #adding a dense layer to output\n",
    "  x = layers.Dense(units = rnn_units * 2, name = \"dense_1\")(x)\n",
    "  x = layers.ReLU(name = \"dense_1_relu\")(x)\n",
    "    #preventing overfitting and adding noise \n",
    "  x = layers.Dropout ( rate = 0.5 )(x)\n",
    "  output = layers.Dense(units = outputdim + 1, activation = \"softmax\")(x)\n",
    "    #creating a model and naming it \n",
    "  model = keras.Model(input_spectrogram, output, name = \"STT_Model\")\n",
    "#adding an optimizer, Adam, for training of model\n",
    "  opt = Adam(learning_rate= 1e-4) \n",
    "  model.compile(optimizer = opt, loss= CTCloss)\n",
    "  return model\n",
    "fft_length = 384\n",
    "#creating an instance of the model\n",
    "model = draft_model(inputdim = fft_length // 2 + 1, outputdim= char_to_num.vocabulary_size(), rnn_units = 512,)\n",
    "model.summary(line_length = 110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bb6034-b2bf-4344-9008-3546f83b1a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoding the predictions made by the model \n",
    "def decode_batch_predictions (pred):\n",
    "    #calculating the length of input sequence in a sample\n",
    "  input_len = np.ones(pred.shape[0]) * pred.shape [1]\n",
    "#decoding the CTC predictions using keras ctc_decode backend function; using the greedy decoder\n",
    "  results = keras.backend.ctc_decode(pred, input_length = input_len, greedy = True)[0][0]\n",
    "    #initiallizing list for the output\n",
    "  output_text =[]\n",
    "  for result in results :\n",
    "        #converting to a NumPy array and then decoding as a UTF-8 string\n",
    "    result = tf.strings.reduce_join(num_to_char(result)).numpy().decode(\"utf-8\")\n",
    "    output_text.append(result)\n",
    "  return output_text\n",
    "#defining a custom callback class to call in at a specific point in the model training to perfom an action \n",
    "class CallbackEval(keras.callbacks.Callback):\n",
    "#initializing the method for the class; self --> instance of the class \n",
    "  def __init__(self, dataset):\n",
    "        #calling the Kears callback function using the super function\n",
    "    super().__init__()\n",
    "    self.dataset = dataset\n",
    "    #defining a specific callback method using at the end of each epoch \n",
    "  def on_epoch_end(self, epoch: int , logs = None):\n",
    "     predictions = []\n",
    "     targets = []\n",
    "     for batch in self.dataset:\n",
    "            #unloading the input (X) and the target (y), into batch \n",
    "      X, y = batch\n",
    "    #using the predict function to get the precditions from the model \n",
    "      batch_predictions = model.predict(X)\n",
    "        #decoding the predictions \n",
    "      batch_predictions = decode_batch_predictions (batch_predictions)\n",
    "    #appending the predictions to the list \n",
    "      predictions.extend(batch_predictions)\n",
    "      for label in y :\n",
    "       label = (tf.strings.reduce_join (num_to_char(label)).numpy().decode(\"utf-8\"))\n",
    "       targets.append (label)\n",
    "        #using the wer function in jiwer to get the word error rate \n",
    "     wer_score = wer (targets, predictions)\n",
    "     print (\".\" *100)\n",
    "     print (f\"Word error rate : { wer_score: .4f}\")\n",
    "     print (\".\"*100)\n",
    "   #printing two of the predictions at random\n",
    "     for i in np.random.randint (0, len(predictions),2):\n",
    "      print (\"target : \", (targets[i]))\n",
    "      print (\"prediction: \", (predictions[i]))\n",
    "      print (\".\" *100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0a1d19-833d-4069-a2f8-a32f206ee366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifying the number of epochs for training \n",
    "epochs = 1\n",
    "#calling the custom Callbackeval function \n",
    "validation_callback = CallbackEval(test_dataset)\n",
    "#training the model using the fit function in keras\n",
    "history = model.fit(train_dataset, validation_data= test_dataset, epochs= epochs, callbacks = [validation_callback],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad3ef3a-f228-488c-921f-f510f4e6fa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for the predictions in the test_dataset \n",
    "predictions = []\n",
    "targets = []\n",
    "for batch in test_dataset :\n",
    "  X, y = batch\n",
    "  batch_predictions = model.predict(X)\n",
    "  batch_predictions = decode_batch_predictions (batch_predictions)\n",
    "  predictions.extend(batch_predictions)\n",
    "  for label in y :\n",
    "       label = (tf.strings.reduce_join (num_to_char(label)).numpy().decode(\"utf-8\"))\n",
    "       targets.append (label)\n",
    "wer_score = wer (targets, predictions)\n",
    "print (\".\" *100)\n",
    "print (f\"Word error rate : { wer_score: .4f}\")\n",
    "print (\".\"*100)\n",
    "print (\".\" *100)\n",
    "# Check if the loop is being executed\n",
    "print(\"Printing random samples:\")\n",
    "for i in range(2):\n",
    "    random_index = np.random.randint(0, len(predictions))\n",
    "    print(\"Random Index:\", random_index)\n",
    "    print(\"target : \", (targets[random_index]))\n",
    "    print(\"prediction: \", (predictions[random_index]))\n",
    "    print(\".\" * 100)\n",
    "for i in np.random.randint (0, len(predictions),2):\n",
    "      print (f\"target : {targets[i]}\")\n",
    "      print (f\"prediction:  {predictions[i]}\")\n",
    "      print (\".\" *100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9b7857-734d-4d37-b6a2-5915a73f2c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model and its assets\n",
    "model.save(\"stt.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cdec86-7fdb-4f2f-b710-7279186939ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
